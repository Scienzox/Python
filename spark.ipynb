{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T06:29:16.272554Z",
     "start_time": "2020-04-08T06:29:12.162694Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "········\n"
     ]
    }
   ],
   "source": [
    "#kinit command in Python\n",
    "from subprocess import Popen, PIPE\n",
    "import getpass\n",
    "userid=getpass.getuser()\n",
    "password = getpass.getpass()\n",
    "kinit = '/usr/bin/kinit'\n",
    "kinit_args = [ kinit, '%s' % (userid) ]\n",
    "kinit_args = [ kinit ]\n",
    "kinit = Popen(kinit_args, stdin=PIPE, stdout=PIPE, stderr=PIPE,universal_newlines=True)\n",
    "kinit.stdin.write('%s\\n' % password)\n",
    "#kinit.wait()\n",
    "out,err = kinit.communicate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T06:29:52.894526Z",
     "start_time": "2020-04-08T06:29:18.267180Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " @       Welcome to  Proximus      __     __  @\n",
      " @    ____              __        /  \\   /  \\ @\n",
      " @   / __/__  ___ _____/ /__      \\___|_|___/ @\n",
      " @  _\\ \\/ _ \\/ _ `/ __/  '_/       ___|_|___  @\n",
      " @ /__ / .__/\\_,_/_/ /_/\\_\\       /   | |   \\ @\n",
      " @    /_/                         \\__/   \\__/ @\n",
      "      version 2.3.0.2.6.5.0-292\n",
      "\n",
      "Using Python version 3.5.5 (default, May 13 2018 21:12:35)\n",
      "Using environment: cmi-py3_0.1\n",
      "Spark session available as <runspark_object>.spark,\n",
      "SparkContext available as <runspark_object>.sc,\n",
      "SQLContext available as <runspark_object>.sqlCtx.\n"
     ]
    }
   ],
   "source": [
    "from bdpcommon.runspark import Runspark \n",
    "\n",
    "conf = {'app_name': 'TEST', 'spark.yarn.queue': 'DEV', 'spark.driver.memory': '15g',\n",
    "        'spark.driver.maxResultSize': '2g',\n",
    "        'spark.port.maxRetries': 50,\n",
    "        'spark.executor.instances' : 2, 'spark.executor.memory' : '2g', 'spark.executor.cores': 2}\n",
    "rp = Runspark(conf=conf, env_name = \"cmi-py3_0.1\", cluster_name=\"dev-el3207\", spark_version=\"2.3.0\")\n",
    "\n",
    "sqlContext = rp.sqlCtx\n",
    "sc = rp.sc\n",
    "spark = rp.spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T06:58:00.394263Z",
     "start_time": "2020-04-08T06:58:00.390745Z"
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"app_name\", \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T06:58:01.737608Z",
     "start_time": "2020-04-08T06:58:01.732243Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: TEST\n",
      "Driver TCP port: 45157\n",
      "Number of partitions: 200\n"
     ]
    }
   ],
   "source": [
    "app_name = spark.conf.get('spark.app.name')\n",
    "\n",
    "driver_tcp_port = spark.conf.get('spark.driver.port')\n",
    "\n",
    "num_partitions = spark.conf.get('spark.sql.shuffle.partitions')\n",
    "\n",
    "print(\"Name: %s\" % app_name)\n",
    "print(\"Driver TCP port: %s\" % driver_tcp_port)\n",
    "print(\"Number of partitions: %s\" % num_partitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import tricks:\n",
    "* Spark prefer many smaller files than one big\n",
    "* if not possible read it, then write it to parquet -> the next import will be faster\n",
    "* can use the * to import multiple files at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T11:09:03.781469Z",
     "start_time": "2020-04-07T11:09:02.968998Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the CSV file\n",
    "aa_dfw_df = spark.read.format('csv').options(Header=True).load('AA_DFW_2018.csv.gz')\n",
    "\n",
    "# Add the airport column using the F.lower() method\n",
    "aa_dfw_df = aa_dfw_df.withColumn('airport', F.lower(aa_dfw_df['Destination Airport']))\n",
    "\n",
    "# Drop the Destination Airport column\n",
    "aa_dfw_df = aa_dfw_df.drop(aa_dfw_df['Destination Airport'])\n",
    "\n",
    "# Show the DataFrame\n",
    "aa_dfw_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Parquet file into flights_df\n",
    "flights_df = spark.read.parquet(\"AA_DFW_ALL.parquet\")\n",
    "\n",
    "# Register the temp table\n",
    "flights_df.createOrReplaceTempView('flights')\n",
    "\n",
    "# Run a SQL query of the average flight duration\n",
    "avg_duration = spark.sql('SELECT avg(flight_duration) from flights').collect()[0]\n",
    "print('The average flight time is: %d' % avg_duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T13:56:27.782866Z",
     "start_time": "2020-04-07T13:56:27.760911Z"
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voter_df.select(\"VOTER_NAME\").distinct().show(40, truncate=False)\n",
    "\n",
    "voter_df = voter_df.filter('length(VOTER_NAME) > 0 and length(VOTER_NAME) < 20')\n",
    "\n",
    "voter_df = voter_df.filter(~ F.col('VOTER_NAME').contains(\"_\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voter_df = voter_df.withColumn(\"splits\", F.split(voter_df.VOTER_NAME, '\\s+'))\n",
    "\n",
    "voter_df = voter_df.withColumn(\"first_name\", voter_df.splits.getItem(0))\n",
    "\n",
    "voter_df = voter_df.withColumn(\"last_name\", voter_df.splits.getItem(F.size('splits') - 1))\n",
    "\n",
    "voter_df = voter_df.drop('splits')\n",
    "\n",
    "voter_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voter_df = voter_df.withColumn('random_val',\n",
    "                               F.when(voter_df.TITLE == 'Councilmember', F.rand())\n",
    "                               .when(voter_df.TITLE == \"Mayor\", 2)\n",
    "                               .otherwise(0))\n",
    "\n",
    "voter_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voter_df = df.select(df[\"VOTER NAME\"]).distinct()\n",
    "\n",
    "print(\"\\nThere are %d rows in the voter_df DataFrame.\\n\" % voter_df.count())\n",
    "\n",
    "voter_df = voter_df.withColumn('ROW_ID', F.monotonically_increasing_id())\n",
    "\n",
    "voter_df.orderBy(voter_df.ROW_ID.desc()).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voter_df = voter_df.distinct().cache() # improve performance\n",
    "voter_df.count()\n",
    "\n",
    "voter_df.is_cached\n",
    "\n",
    "vector_df.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the broadcast method from pyspark.sql.functions\n",
    "from pyspark.sql.functions import broadcast # improve joins when small datasets\n",
    "\n",
    "# Join the flights_df and airports_df DataFrames using broadcasting\n",
    "broadcast_df = flights_df.join(broadcast(airports_df), \\\n",
    "    flights_df[\"Destination Airport\"] == airports_df[\"IATA\"] )\n",
    "\n",
    "# Show the query plan and compare against the original\n",
    "broadcast_df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFirstAndMiddle(names):\n",
    "  return ' '.join(names)\n",
    "\n",
    "udfFirstAndMiddle = F.udf(getFirstAndMiddle, StringType())\n",
    "\n",
    "voter_df = voter_df.withColumn('first_and_middle_name', udfFirstAndMiddle(voter_df.splits))\n",
    "\n",
    "voter_df = voter_df.drop(\"first_name\")\n",
    "voter_df = voter_df.drop(\"splits\")\n",
    "voter_df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5",
   "language": "python",
   "name": "my_python_3.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
