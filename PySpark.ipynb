{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few words about Spark\n",
    "\n",
    "* **SparkContext** = connection to the cluster\n",
    "* **SparkSession** = interface with that connection\n",
    "\n",
    "* **RDD** = Resilient Distributed DataFrames. Low level, more complicated to work with\n",
    "* **Spark df** = easier to understand and work with \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark outside Proximus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SparkSession from pyspark.sql\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create my_spark\n",
    "my_spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Print my_spark\n",
    "print(my_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the tables in the catalog\n",
    "print(spark.catalog.listTables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't change this query\n",
    "query = \"FROM flights SELECT * LIMIT 10\"\n",
    "\n",
    "# Get the first 10 rows of flights\n",
    "flights10 = spark.sql(query)\n",
    "\n",
    "# Show the results\n",
    "flights10.show()\n",
    "\n",
    "pandas_df = flights10.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pd_temp\n",
    "pd_temp = pd.DataFrame(np.random.random(10))\n",
    "\n",
    "# Create spark_temp from pd_temp\n",
    "spark_temp = spark.createDataFrame(pd_temp)\n",
    "\n",
    "# Examine the tables in the catalog\n",
    "print(spark.catalog.listTables())\n",
    "\n",
    "# Add spark_temp to the catalog\n",
    "spark_temp.createOrReplaceTempView(\"temp\")\n",
    "\n",
    "# Examine the tables in the catalog again\n",
    "print(spark.catalog.listTables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't change this file path\n",
    "file_path = \"/usr/local/share/datasets/airports.csv\"\n",
    "\n",
    "# Read in the airports data\n",
    "airports = spark.read.csv(file_path, header=True)\n",
    "\n",
    "# Show the data\n",
    "airports.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DataFrame flights\n",
    "flights = spark.table(\"flights\")\n",
    "\n",
    "# Show the head\n",
    "print(flights.show())\n",
    "\n",
    "# Add duration_hrs\n",
    "flights = flights.withColumn(\"duration_hrs\", flights.air_time/60)\n",
    "\n",
    "long_flights1 = flights.filter(\"distance > 1000\")\n",
    "\n",
    "# Select the first set of columns\n",
    "selected1 = flights.select(\"tailnum\", \"origin\", \"dest\")\n",
    "\n",
    "flights.filter(flights.origin == \"PDX\").groupBy().min(\"distance\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define avg_speed\n",
    "avg_speed = (flights.distance/(flights.air_time/60)).alias(\"avg_speed\")\n",
    "\n",
    "# Select the correct columns\n",
    "speed1 = flights.select(\"origin\", \"dest\", \"tailnum\", avg_speed)\n",
    "\n",
    "# Create the same table using a SQL expression\n",
    "speed2 = flights.selectExpr(\"origin\", \"dest\", \"tailnum\", \"distance/(air_time/60) as avg_speed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pyspark.sql.functions as F\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Group by month and dest\n",
    "by_month_dest = flights.groupBy(\"month\", \"dest\")\n",
    "\n",
    "# Standard deviation\n",
    "by_month_dest.agg(F.stddev(\"dep_delay\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark context initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T08:43:52.738371Z",
     "start_time": "2020-02-13T08:43:49.608841Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "········\n"
     ]
    }
   ],
   "source": [
    "#kinit command in Python\n",
    "from subprocess import Popen, PIPE\n",
    "import getpass\n",
    "userid=getpass.getuser()\n",
    "password = getpass.getpass()\n",
    "kinit = '/usr/bin/kinit'\n",
    "kinit_args = [ kinit, '%s' % (userid) ]\n",
    "kinit_args = [ kinit ]\n",
    "kinit = Popen(kinit_args, stdin=PIPE, stdout=PIPE, stderr=PIPE,universal_newlines=True)\n",
    "kinit.stdin.write('%s\\n' % password)\n",
    "#kinit.wait()\n",
    "out,err = kinit.communicate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T08:44:18.780305Z",
     "start_time": "2020-02-13T08:43:54.824244Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " @       Welcome to  Proximus      __     __  @\n",
      " @    ____              __        /  \\   /  \\ @\n",
      " @   / __/__  ___ _____/ /__      \\___|_|___/ @\n",
      " @  _\\ \\/ _ \\/ _ `/ __/  '_/       ___|_|___  @\n",
      " @ /__ / .__/\\_,_/_/ /_/\\_\\       /   | |   \\ @\n",
      " @    /_/                         \\__/   \\__/ @\n",
      "      version 2.3.0.2.6.5.0-292\n",
      "\n",
      "Using Python version 2.7.15 (default, Nov 29 2018 06:43:57)\n",
      "Spark session available as <runspark_object>.spark,\n",
      "SparkContext available as <runspark_object>.sc,\n",
      "SQLContext available as <runspark_object>.sqlCtx.\n"
     ]
    }
   ],
   "source": [
    "from bdpcommon.runspark import Runspark\n",
    "from functools import reduce\n",
    "\n",
    "config = {'spark.executor.instances' : 8,\n",
    "            'spark.executor.memory' : '2g',\n",
    "            'spark.driver.memory' : '2g',\n",
    "            'spark.executor.cores' : '4',\n",
    "          'spark.app.name':'TESTID860112',\n",
    "          'spark.yarn.queue': 'DEV',\n",
    "          'spark.port.maxRetries': 100}\n",
    "    \n",
    "    \n",
    "#rp = Runspark(cluster_name=\"dev-el3207\", conf = config , spark_version=\"2.3.0\")\n",
    "\n",
    "\n",
    "# rp = Runspark(cluster_name=\"prod-datalake\", conf = config ,spark_version=\"1.6.3\") \n",
    "rp = Runspark(cluster_name=\"prod-datalake\", conf = config ,spark_version=\"2.3.0\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T08:44:18.805932Z",
     "start_time": "2020-02-13T08:44:18.785276Z"
    }
   },
   "outputs": [],
   "source": [
    "sc = rp.sc\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "sqlContext = SQLContext(rp.sc)\n",
    "hiveContext = rp.sqlContext "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T08:44:18.816176Z",
     "start_time": "2020-02-13T08:44:18.810522Z"
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import col, when, isnull\n",
    "\n",
    "from pyspark.sql import DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-20T11:15:34.732799Z",
     "start_time": "2019-06-20T11:15:34.727483Z"
    }
   },
   "source": [
    "# Spark "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-20T11:15:42.210190Z",
     "start_time": "2019-06-20T11:15:42.190028Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'2.3.0.2.6.5.0-292'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-20T11:15:51.955702Z",
     "start_time": "2019-06-20T11:15:51.948253Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.7'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.pythonVer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-20T11:15:57.813613Z",
     "start_time": "2019-06-20T11:15:57.806204Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'yarn-client'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T08:46:42.728743Z",
     "start_time": "2020-02-13T08:46:42.716794Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [1,2,5,8,10,11]\n",
    "rdd = sc.parallelize(data)\n",
    "type(rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T08:45:28.270133Z",
     "start_time": "2020-02-13T08:45:27.702543Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 5, 8, 10, 11]\n",
      "[1, 2]\n",
      "1\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "print(rdd.collect())\n",
    "print(rdd.take(2))\n",
    "print(rdd.first())\n",
    "print(rdd.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T08:45:29.042844Z",
     "start_time": "2020-02-13T08:45:28.894919Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 4, 7, 10, 12, 13]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.map(lambda x : x+2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T08:45:29.514883Z",
     "start_time": "2020-02-13T08:45:29.508105Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T08:45:30.775241Z",
     "start_time": "2020-02-13T08:45:30.726129Z"
    }
   },
   "outputs": [],
   "source": [
    "fileRDD = sc.textFile(\"Data/text.txt\")\n",
    "# fileRDD.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T08:49:04.940611Z",
     "start_time": "2020-02-13T08:49:04.857222Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 25, 64, 100, 121]\n",
      "[1, 4, 25, 64, 100, 121]\n"
     ]
    }
   ],
   "source": [
    "print(map(lambda x: x**2, data))\n",
    "print(rdd.map(lambda x:x**2).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T08:49:03.654334Z",
     "start_time": "2020-02-13T08:49:03.535972Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 8, 10]\n",
      "[2, 8, 10]\n"
     ]
    }
   ],
   "source": [
    "print(filter(lambda x: x%2 == 0, data))\n",
    "print(rdd.filter(lambda x:x%2==0).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T08:49:19.010789Z",
     "start_time": "2020-02-13T08:49:18.811236Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n",
      "37\n"
     ]
    }
   ],
   "source": [
    "print(reduce(lambda x,y: x+y, data))\n",
    "print(rdd.reduce(lambda x,y: x+y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T08:49:48.690811Z",
     "start_time": "2020-02-13T08:49:48.601377Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'world', 'how', 'are', 'you', 'Ben']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([\"Hello world how are you\", \"Ben\"])\n",
    "rdd.flatMap(lambda x:x.split()).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pair RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T08:52:55.425022Z",
     "start_time": "2020-02-13T08:52:55.152541Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ben', 29), ('noumi', 23), ('ben', 40)]\n",
      "[('ben', 69), ('noumi', 23)]\n"
     ]
    }
   ],
   "source": [
    "x = [(\"ben\",29), (\"noumi\", 23), (\"ben\", 40)]\n",
    "paired_rdd = sc.parallelize(x)\n",
    "print(paired_rdd.collect())\n",
    "print(paired_rdd.reduceByKey(lambda x,y : x + y).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T08:54:20.647536Z",
     "start_time": "2020-02-13T08:54:20.049079Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(69, 'ben'), (23, 'noumi')]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paired_rdd.reduceByKey(lambda x,y : x + y)\\\n",
    "    .map(lambda x: (x[1], x[0]))\\\n",
    "    .sortByKey(ascending=False)\\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T08:56:10.556725Z",
     "start_time": "2020-02-13T08:56:10.219776Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Neymar', (24, 120)), ('Ronaldo', (32, 80)), ('Messi', (34, 100))]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RDD1 = sc.parallelize([(\"Messi\", 34),(\"Ronaldo\", 32),(\"Neymar\", 24)])\n",
    "RDD2 = sc.parallelize([(\"Ronaldo\", 80),(\"Neymar\", 120),(\"Messi\", 100)])\n",
    "\n",
    "RDD1.join(RDD2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T08:57:47.973821Z",
     "start_time": "2020-02-13T08:57:47.874447Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('a', 2)\n",
      "('b', 1)\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 2)])\n",
    "for kee, val in rdd.countByKey().items():\n",
    "    print(kee, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T08:58:06.515722Z",
     "start_time": "2020-02-13T08:58:06.479548Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 2, 3: 4}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([(1, 2), (3, 4)]).collectAsMap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T09:14:10.299370Z",
     "start_time": "2020-02-13T09:14:09.564274Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "[Row(Model=u'XS', Year=2018, Height=5.65, Width=2.79, Weight=6.24), Row(Model=u'XR', Year=2018, Height=5.94, Width=2.98, Weight=6.84), Row(Model=u'X10', Year=2017, Height=5.65, Width=2.79, Weight=6.13), Row(Model=u'8Plus', Year=2017, Height=6.23, Width=3.07, Weight=7.12)]\n",
      "+-----+----+------+-----+------+\n",
      "|Model|Year|Height|Width|Weight|\n",
      "+-----+----+------+-----+------+\n",
      "|   XS|2018|  5.65| 2.79|  6.24|\n",
      "|   XR|2018|  5.94| 2.98|  6.84|\n",
      "|  X10|2017|  5.65| 2.79|  6.13|\n",
      "|8Plus|2017|  6.23| 3.07|  7.12|\n",
      "+-----+----+------+-----+------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "iphones_RDD = sc.parallelize([\n",
    "    (\"XS\", 2018, 5.65, 2.79, 6.24),\n",
    "    (\"XR\", 2018, 5.94, 2.98, 6.84),\n",
    "    (\"X10\", 2017, 5.65, 2.79, 6.13),\n",
    "    (\"8Plus\", 2017, 6.23, 3.07, 7.12)\n",
    "    ])\n",
    "\n",
    "names = [ 'Model',\n",
    "    'Year',\n",
    "    'Height',\n",
    "    'Width',\n",
    "    'Weight'\n",
    "    ]\n",
    "\n",
    "iphones_df = sqlContext.createDataFrame(iphones_RDD, schema=names)\n",
    "\n",
    "print(type(iphones_df))\n",
    "print(iphones_df.collect())\n",
    "print(iphones_df.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T09:16:00.510029Z",
     "start_time": "2020-02-13T09:16:00.461431Z"
    }
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "u'Path does not exist: hdfs://DATALAKEHA/user/id860112/Playground/Data/data.csv;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-108-830cc2620eb7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_csv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqlContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Playground/Data/data.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minferSchema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/px/devs/miniconda/envs/spark_2.3.0/share/spark-2.3.0/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping)\u001b[0m\n\u001b[1;32m    437\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/jupyterhub/id860112/envs/Python_2.7/lib/python2.7/site-packages/py4j/java_gateway.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1284\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1286\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/px/devs/miniconda/envs/spark_2.3.0/share/spark-2.3.0/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: u'Path does not exist: hdfs://DATALAKEHA/user/id860112/Playground/Data/data.csv;'"
     ]
    }
   ],
   "source": [
    "df_csv = sqlContext.read.csv(\"Playground/Data/data.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# error because it looks on the datalake, not current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T09:17:41.888605Z",
     "start_time": "2020-02-13T09:17:41.853546Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num</th>\n",
       "      <th>Name</th>\n",
       "      <th>Total</th>\n",
       "      <th>HP</th>\n",
       "      <th>Attack</th>\n",
       "      <th>Defense</th>\n",
       "      <th>Sp. Atk</th>\n",
       "      <th>Sp. Def</th>\n",
       "      <th>Speed</th>\n",
       "      <th>Generation</th>\n",
       "      <th>Legendary</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Bulbasaur</td>\n",
       "      <td>318</td>\n",
       "      <td>45</td>\n",
       "      <td>49</td>\n",
       "      <td>49</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>Grass-Poison</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Ivysaur</td>\n",
       "      <td>405</td>\n",
       "      <td>60</td>\n",
       "      <td>62</td>\n",
       "      <td>63</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>Grass-Poison</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Venusaur</td>\n",
       "      <td>525</td>\n",
       "      <td>80</td>\n",
       "      <td>82</td>\n",
       "      <td>83</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>Grass-Poison</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>VenusaurMega Venusaur</td>\n",
       "      <td>625</td>\n",
       "      <td>80</td>\n",
       "      <td>100</td>\n",
       "      <td>123</td>\n",
       "      <td>122</td>\n",
       "      <td>120</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>Grass-Poison</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Charmander</td>\n",
       "      <td>309</td>\n",
       "      <td>39</td>\n",
       "      <td>52</td>\n",
       "      <td>43</td>\n",
       "      <td>60</td>\n",
       "      <td>50</td>\n",
       "      <td>65</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>Fire-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num                   Name  Total  HP  Attack  Defense  Sp. Atk  Sp. Def  \\\n",
       "0    1              Bulbasaur    318  45      49       49       65       65   \n",
       "1    2                Ivysaur    405  60      62       63       80       80   \n",
       "2    3               Venusaur    525  80      82       83      100      100   \n",
       "3    3  VenusaurMega Venusaur    625  80     100      123      122      120   \n",
       "4    4             Charmander    309  39      52       43       60       50   \n",
       "\n",
       "   Speed  Generation  Legendary          Type  \n",
       "0     45           1      False  Grass-Poison  \n",
       "1     60           1      False  Grass-Poison  \n",
       "2     80           1      False  Grass-Poison  \n",
       "3     80           1      False  Grass-Poison  \n",
       "4     65           1      False        Fire-   "
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"Data/data.csv\")\n",
    "\n",
    "df[\"Type 2\"].fillna(\" \", inplace=True)\n",
    "df[\"Type\"] = df[\"Type 1\"] + \"-\" + df[\"Type 2\"]\n",
    "df.drop([\"Type 1\", \"Type 2\"], axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T09:17:43.744571Z",
     "start_time": "2020-02-13T09:17:43.613015Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- num: long (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Total: long (nullable = true)\n",
      " |-- HP: long (nullable = true)\n",
      " |-- Attack: long (nullable = true)\n",
      " |-- Defense: long (nullable = true)\n",
      " |-- Sp. Atk: long (nullable = true)\n",
      " |-- Sp. Def: long (nullable = true)\n",
      " |-- Speed: long (nullable = true)\n",
      " |-- Generation: long (nullable = true)\n",
      " |-- Legendary: boolean (nullable = true)\n",
      " |-- Type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf = sqlContext.createDataFrame(df)\n",
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T09:19:30.810277Z",
     "start_time": "2020-02-13T09:19:30.795911Z"
    }
   },
   "outputs": [],
   "source": [
    "sdf = sdf.withColumnRenamed(\"Sp. Atk\", \"Spec_att\")\n",
    "sdf = sdf.withColumnRenamed(\"Sp. Def\", \"Spec_def\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T09:19:31.451573Z",
     "start_time": "2020-02-13T09:19:31.364335Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-----+---+------+-------+--------+--------+-----+----------+---------+------------+\n",
      "|num|                Name|Total| HP|Attack|Defense|Spec_att|Spec_def|Speed|Generation|Legendary|        Type|\n",
      "+---+--------------------+-----+---+------+-------+--------+--------+-----+----------+---------+------------+\n",
      "|  1|           Bulbasaur|  318| 45|    49|     49|      65|      65|   45|         1|    false|Grass-Poison|\n",
      "|  2|             Ivysaur|  405| 60|    62|     63|      80|      80|   60|         1|    false|Grass-Poison|\n",
      "|  3|            Venusaur|  525| 80|    82|     83|     100|     100|   80|         1|    false|Grass-Poison|\n",
      "|  3|VenusaurMega Venu...|  625| 80|   100|    123|     122|     120|   80|         1|    false|Grass-Poison|\n",
      "|  4|          Charmander|  309| 39|    52|     43|      60|      50|   65|         1|    false|      Fire- |\n",
      "+---+--------------------+-----+---+------+-------+--------+--------+-----+----------+---------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T09:19:53.327326Z",
     "start_time": "2020-02-13T09:19:53.299601Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['num',\n",
       " 'Name',\n",
       " 'Total',\n",
       " 'HP',\n",
       " 'Attack',\n",
       " 'Defense',\n",
       " 'Spec_att',\n",
       " 'Spec_def',\n",
       " 'Speed',\n",
       " 'Generation',\n",
       " 'Legendary',\n",
       " 'Type']"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T09:20:06.103188Z",
     "start_time": "2020-02-13T09:20:05.350242Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+----------------+----------------+-----------------+-----------------+-----------------+----------------+------------------+------------------+------------------+-----------+\n",
      "|summary|               num|            Name|           Total|               HP|           Attack|          Defense|        Spec_att|          Spec_def|             Speed|        Generation|       Type|\n",
      "+-------+------------------+----------------+----------------+-----------------+-----------------+-----------------+----------------+------------------+------------------+------------------+-----------+\n",
      "|  count|               800|             800|             800|              800|              800|              800|             800|               800|               800|               800|        800|\n",
      "|   mean|         362.81375|            null|        435.1025|         69.25875|         79.00125|          73.8425|           72.82|           71.9025|           68.2775|           3.32375|       null|\n",
      "| stddev|208.34379756406665|            null|119.963039755519|25.53466903233207|32.45736586949845|31.18350055933293|32.7222941688016|27.828915797117453|29.060473717161464|1.6612904004849451|       null|\n",
      "|    min|                 1|       Abomasnow|             180|                1|                5|                5|              10|                20|                 5|                 1|      Bug- |\n",
      "|    max|               721|Zygarde50% Forme|             780|              255|              190|              230|             194|               230|               180|                 6|Water-Steel|\n",
      "+-------+------------------+----------------+----------------+-----------------+-----------------+-----------------+----------------+------------------+------------------+------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-29T12:57:49.216226Z",
     "start_time": "2019-04-29T12:57:44.989852Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|Speed|\n",
      "+-----+\n",
      "|   45|\n",
      "|   60|\n",
      "|   80|\n",
      "|   80|\n",
      "|   65|\n",
      "+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.select('Speed').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-29T12:57:58.038470Z",
     "start_time": "2019-04-29T12:57:53.888450Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+---+------+-------+-------+-------+-----+----------+---------+------------+\n",
      "|      Name|Total| HP|Attack|Defense|Sp. Atk|Sp. Def|Speed|Generation|Legendary|        Type|\n",
      "+----------+-----+---+------+-------+-------+-------+-----+----------+---------+------------+\n",
      "| Bulbasaur|  318| 45|    49|     49|     65|     65|   45|         1|    false|Grass-Poison|\n",
      "|   Ivysaur|  405| 60|    62|     63|     80|     80|   60|         1|    false|Grass-Poison|\n",
      "|Charmander|  309| 39|    52|     43|     60|     50|   65|         1|    false|      Fire- |\n",
      "|Charmeleon|  405| 58|    64|     58|     80|     65|   80|         1|    false|      Fire- |\n",
      "|  Squirtle|  314| 44|    48|     65|     50|     64|   43|         1|    false|     Water- |\n",
      "+----------+-----+---+------+-------+-------+-------+-----+----------+---------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.filter(sdf['Attack'] < 80).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-29T14:54:13.977176Z",
     "start_time": "2019-04-29T14:54:13.827970Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+---+------+-------+-------+-------+-----+----------+---------+------------+-------+\n",
      "|                Name|Total| HP|Attack|Defense|Sp. Atk|Sp. Def|Speed|Generation|Legendary|        Type|new col|\n",
      "+--------------------+-----+---+------+-------+-------+-------+-----+----------+---------+------------+-------+\n",
      "|           Bulbasaur|  318| 45|    49|     49|     65|     65|   45|         1|    false|Grass-Poison|   3.18|\n",
      "|             Ivysaur|  405| 60|    62|     63|     80|     80|   60|         1|    false|Grass-Poison|   4.05|\n",
      "|            Venusaur|  525| 80|    82|     83|    100|    100|   80|         1|    false|Grass-Poison|   5.25|\n",
      "|VenusaurMega Venu...|  625| 80|   100|    123|    122|    120|   80|         1|    false|Grass-Poison|   6.25|\n",
      "|          Charmander|  309| 39|    52|     43|     60|     50|   65|         1|    false|      Fire- |   3.09|\n",
      "|          Charmeleon|  405| 58|    64|     58|     80|     65|   80|         1|    false|      Fire- |   4.05|\n",
      "+--------------------+-----+---+------+-------+-------+-------+-----+----------+---------+------------+-------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.withColumn('new col', sdf['Total'] / 100).show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T09:18:15.335453Z",
     "start_time": "2020-02-13T09:18:13.933795Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+\n",
      "|Legendary|        avg(Total)|\n",
      "+---------+------------------+\n",
      "|     true| 637.3846153846154|\n",
      "|    false|417.21360544217686|\n",
      "+---------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.dropDuplicates()\\\n",
    "    .groupby(['Legendary'])\\\n",
    "    .agg({\"Total\": \"AVG\"})\\\n",
    "    .sort(\"avg(Total)\", ascending=False)\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-12T15:43:28.040465Z",
     "start_time": "2020-02-12T15:43:26.036935Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-----+---+------+-------+-------+-------+-----+----------+---------+------------+-------+---------+\n",
      "|num|                Name|Total| HP|Attack|Defense|Sp. Atk|Sp. Def|Speed|Generation|Legendary|        Type|Att_def|is_strong|\n",
      "+---+--------------------+-----+---+------+-------+-------+-------+-----+----------+---------+------------+-------+---------+\n",
      "|  1|           Bulbasaur|  318| 45|    49|     49|     65|     65|   45|         1|    false|Grass-Poison|     98|    false|\n",
      "|  2|             Ivysaur|  405| 60|    62|     63|     80|     80|   60|         1|    false|Grass-Poison|    125|    false|\n",
      "|  3|            Venusaur|  525| 80|    82|     83|    100|    100|   80|         1|    false|Grass-Poison|    165|    false|\n",
      "|  3|VenusaurMega Venu...|  625| 80|   100|    123|    122|    120|   80|         1|    false|Grass-Poison|    223|     true|\n",
      "|  4|          Charmander|  309| 39|    52|     43|     60|     50|   65|         1|    false|      Fire- |     95|    false|\n",
      "+---+--------------------+-----+---+------+-------+-------+-------+-----+----------+---------+------------+-------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf2 = sdf.withColumn(\"Att_def\", sdf.Attack + sdf.Defense)\n",
    "sdf2 = sdf2.withColumn(\"is_strong\", sdf2.Total > 550)\n",
    "sdf2.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T10:00:21.229410Z",
     "start_time": "2020-02-13T10:00:20.516410Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|Generation|        avg(Total)|\n",
      "+----------+------------------+\n",
      "|         1|426.81325301204816|\n",
      "|         2| 418.2830188679245|\n",
      "|         3|           436.225|\n",
      "|         4| 459.0165289256198|\n",
      "|         5| 434.9878787878788|\n",
      "|         6| 436.3780487804878|\n",
      "+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.createOrReplaceTempView(\"table1\")\n",
    "\n",
    "query = sqlContext.sql(\"select Generation, avg(Total) from table1 group by 1 order by 1\")\n",
    "query.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T10:05:09.526762Z",
     "start_time": "2020-02-13T10:05:09.130405Z"
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named matplotlib.pyplot",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mImportError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-142-21ce3f2337c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Total\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"density\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named matplotlib.pyplot"
     ]
    }
   ],
   "source": [
    "sdf.select(\"Total\").toPandas().plot(kind=\"density\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T10:07:25.499199Z",
     "start_time": "2020-02-13T10:07:24.772356Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'toHandy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-146-b5271cbfebdf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoHandy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mhdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Total\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/px/devs/miniconda/envs/spark_2.3.0/share/spark-2.3.0/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m             raise AttributeError(\n\u001b[0;32m-> 1182\u001b[0;31m                 \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n\u001b[0m\u001b[1;32m   1183\u001b[0m         \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'toHandy'"
     ]
    }
   ],
   "source": [
    "hdf = sdf.toHandy() # need to install HandySpark\n",
    "hdf.cols[\"Total\"].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-12T16:44:45.286472Z",
     "start_time": "2020-02-12T16:44:44.378674Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "sdf = sdf.withColumn(\"label\", sdf.Legendary.cast(\"integer\")) # mandatory to have a label column as integer for ML\n",
    "\n",
    "type_indexer = StringIndexer(inputCol=\"Type\", outputCol=\"type_index\")\n",
    "\n",
    "type_encoder = OneHotEncoder(inputCol = \"type_index\", outputCol = \"type_fact\")\n",
    "\n",
    "vec_assembler = VectorAssembler(inputCols=[\"Total\", \"HP\", \"Attack\", \"Defense\", \"Speed\", \"type_fact\"], outputCol=\"features\")\n",
    "\n",
    "pipeline = Pipeline(stages=[type_indexer, type_encoder, vec_assembler])\n",
    "\n",
    "piped_data = pipeline.fit(sdf).transform(sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-12T16:44:46.555489Z",
     "start_time": "2020-02-12T16:44:46.528718Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "training, test = piped_data.randomSplit([0.7,0.3])\n",
    "\n",
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-12T16:44:58.300080Z",
     "start_time": "2020-02-12T16:44:47.714574Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression_414c809cb6f00af4348a\n"
     ]
    }
   ],
   "source": [
    "import pyspark.ml.evaluation as evals\n",
    "import pyspark.ml.tuning as tune\n",
    "import numpy as np\n",
    "\n",
    "evaluator = evals.BinaryClassificationEvaluator(metricName = \"areaUnderROC\")\n",
    "\n",
    "grid = tune.ParamGridBuilder()\n",
    "\n",
    "grid = grid.addGrid(lr.regParam, np.arange(0, .1, .01))\n",
    "grid = grid.addGrid(lr.elasticNetParam, [0,1])\n",
    "\n",
    "grid = grid.build()\n",
    "\n",
    "cv = tune.CrossValidator(estimator=lr,\n",
    "               estimatorParamMaps=grid,\n",
    "               evaluator=evaluator\n",
    "               )\n",
    "\n",
    "best_lr = lr.fit(training)\n",
    "\n",
    "print(best_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-12T16:47:18.273004Z",
     "start_time": "2020-02-12T16:47:17.592412Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.890950792327\n"
     ]
    }
   ],
   "source": [
    "test_results = best_lr.transform(test)\n",
    "\n",
    "# Evaluate the predictions\n",
    "print(evaluator.evaluate(test_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T10:11:32.066268Z",
     "start_time": "2020-02-13T10:11:31.551725Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.recommendation import ALS\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "from pyspark.mllib.clustering import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T10:13:58.111762Z",
     "start_time": "2020-02-13T10:13:56.041335Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Rating(user=1, product=1, rating=1.0),\n",
       " Rating(user=1, product=2, rating=2.0),\n",
       " Rating(user=2, product=1, rating=2.0)]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.recommendation import Rating\n",
    "\n",
    "r1 = Rating(1, 1, 1.0)\n",
    "r2 = Rating(1, 2, 2.0)\n",
    "r3 = Rating(2, 1, 2.0)\n",
    "ratings = sc.parallelize([r1, r2, r3])\n",
    "ratings.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T10:18:51.938804Z",
     "start_time": "2020-02-13T10:18:49.609429Z"
    }
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 16 in stage 136.0 failed 4 times, most recent failure: Lost task 16.3 in stage 136.0 (TID 3651, el8372.bc, executor 7): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/hadoopfs11/hadoop/yarn/local/usercache/id860112/appcache/application_1580124036614_25546/container_e118_1580124036614_25546_01_000008/pyspark.zip/pyspark/worker.py\", line 229, in main\n    process()\n  File \"/hadoopfs11/hadoop/yarn/local/usercache/id860112/appcache/application_1580124036614_25546/container_e118_1580124036614_25546_01_000008/pyspark.zip/pyspark/worker.py\", line 224, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/hadoopfs11/hadoop/yarn/local/usercache/id860112/appcache/application_1580124036614_25546/container_e118_1580124036614_25546_01_000008/pyspark.zip/pyspark/serializers.py\", line 372, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/opt/px/devs/miniconda/envs/spark_2.3.0/share/spark-2.3.0/python/pyspark/rdd.py\", line 1354, in takeUpToNumLeft\n  File \"/hadoopfs11/hadoop/yarn/local/usercache/id860112/appcache/application_1580124036614_25546/container_e118_1580124036614_25546_01_000008/pyspark.zip/pyspark/serializers.py\", line 145, in load_stream\n    yield self._read_with_length(stream)\n  File \"/hadoopfs11/hadoop/yarn/local/usercache/id860112/appcache/application_1580124036614_25546/container_e118_1580124036614_25546_01_000008/pyspark.zip/pyspark/serializers.py\", line 170, in _read_with_length\n    return self.loads(obj)\n  File \"/hadoopfs11/hadoop/yarn/local/usercache/id860112/appcache/application_1580124036614_25546/container_e118_1580124036614_25546_01_000008/pyspark.zip/pyspark/serializers.py\", line 562, in loads\n    return pickle.loads(obj)\n  File \"/hadoopfs11/hadoop/yarn/local/usercache/id860112/appcache/application_1580124036614_25546/container_e118_1580124036614_25546_01_000008/pyspark.zip/pyspark/mllib/__init__.py\", line 28, in <module>\nImportError: No module named numpy\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$1.apply(PythonRDD.scala:141)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$1.apply(PythonRDD.scala:141)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:141)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor168.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/hadoopfs11/hadoop/yarn/local/usercache/id860112/appcache/application_1580124036614_25546/container_e118_1580124036614_25546_01_000008/pyspark.zip/pyspark/worker.py\", line 229, in main\n    process()\n  File \"/hadoopfs11/hadoop/yarn/local/usercache/id860112/appcache/application_1580124036614_25546/container_e118_1580124036614_25546_01_000008/pyspark.zip/pyspark/worker.py\", line 224, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/hadoopfs11/hadoop/yarn/local/usercache/id860112/appcache/application_1580124036614_25546/container_e118_1580124036614_25546_01_000008/pyspark.zip/pyspark/serializers.py\", line 372, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/opt/px/devs/miniconda/envs/spark_2.3.0/share/spark-2.3.0/python/pyspark/rdd.py\", line 1354, in takeUpToNumLeft\n  File \"/hadoopfs11/hadoop/yarn/local/usercache/id860112/appcache/application_1580124036614_25546/container_e118_1580124036614_25546_01_000008/pyspark.zip/pyspark/serializers.py\", line 145, in load_stream\n    yield self._read_with_length(stream)\n  File \"/hadoopfs11/hadoop/yarn/local/usercache/id860112/appcache/application_1580124036614_25546/container_e118_1580124036614_25546_01_000008/pyspark.zip/pyspark/serializers.py\", line 170, in _read_with_length\n    return self.loads(obj)\n  File \"/hadoopfs11/hadoop/yarn/local/usercache/id860112/appcache/application_1580124036614_25546/container_e118_1580124036614_25546_01_000008/pyspark.zip/pyspark/serializers.py\", line 562, in loads\n    return pickle.loads(obj)\n  File \"/hadoopfs11/hadoop/yarn/local/usercache/id860112/appcache/application_1580124036614_25546/container_e118_1580124036614_25546_01_000008/pyspark.zip/pyspark/mllib/__init__.py\", line 28, in <module>\nImportError: No module named numpy\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$1.apply(PythonRDD.scala:141)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$1.apply(PythonRDD.scala:141)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-162-b01da3dea3b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mALS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mratings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# unrated_RDD = sc.parallelize([(1, 2), (1, 1)])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# predictions = model.predictAll(unrated_RDD)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/px/devs/miniconda/envs/spark_2.3.0/share/spark-2.3.0/python/pyspark/mllib/recommendation.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(cls, ratings, rank, iterations, lambda_, blocks, nonnegative, seed)\u001b[0m\n\u001b[1;32m    270\u001b[0m           \u001b[0;34m(\u001b[0m\u001b[0mdefault\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         \"\"\"\n\u001b[0;32m--> 272\u001b[0;31m         model = callMLlibFunc(\"trainALSModel\", cls._prepare(ratings), rank, iterations,\n\u001b[0m\u001b[1;32m    273\u001b[0m                               lambda_, blocks, nonnegative, seed)\n\u001b[1;32m    274\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mMatrixFactorizationModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/px/devs/miniconda/envs/spark_2.3.0/share/spark-2.3.0/python/pyspark/mllib/recommendation.py\u001b[0m in \u001b[0;36m_prepare\u001b[0;34m(cls, ratings)\u001b[0m\n\u001b[1;32m    227\u001b[0m             raise TypeError(\"Ratings should be represented by either an RDD or a DataFrame, \"\n\u001b[1;32m    228\u001b[0m                             \"but got %s.\" % type(ratings))\n\u001b[0;32m--> 229\u001b[0;31m         \u001b[0mfirst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mratings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRating\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/px/devs/miniconda/envs/spark_2.3.0/share/spark-2.3.0/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mfirst\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1374\u001b[0m         \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRDD\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mempty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1375\u001b[0m         \"\"\"\n\u001b[0;32m-> 1376\u001b[0;31m         \u001b[0mrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1378\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/px/devs/miniconda/envs/spark_2.3.0/share/spark-2.3.0/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1358\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/px/devs/miniconda/envs/spark_2.3.0/share/spark-2.3.0/python/pyspark/context.py\u001b[0m in \u001b[0;36mrunJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m    999\u001b[0m         \u001b[0;31m# SparkContext#runJob.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1000\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1001\u001b[0;31m         \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1002\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1003\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/jupyterhub/id860112/envs/Python_2.7/lib/python2.7/site-packages/py4j/java_gateway.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1284\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1286\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/px/devs/miniconda/envs/spark_2.3.0/share/spark-2.3.0/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/jupyterhub/id860112/envs/Python_2.7/lib/python2.7/site-packages/py4j/protocol.pyc\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 16 in stage 136.0 failed 4 times, most recent failure: Lost task 16.3 in stage 136.0 (TID 3651, el8372.bc, executor 7): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/hadoopfs11/hadoop/yarn/local/usercache/id860112/appcache/application_1580124036614_25546/container_e118_1580124036614_25546_01_000008/pyspark.zip/pyspark/worker.py\", line 229, in main\n    process()\n  File \"/hadoopfs11/hadoop/yarn/local/usercache/id860112/appcache/application_1580124036614_25546/container_e118_1580124036614_25546_01_000008/pyspark.zip/pyspark/worker.py\", line 224, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/hadoopfs11/hadoop/yarn/local/usercache/id860112/appcache/application_1580124036614_25546/container_e118_1580124036614_25546_01_000008/pyspark.zip/pyspark/serializers.py\", line 372, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/opt/px/devs/miniconda/envs/spark_2.3.0/share/spark-2.3.0/python/pyspark/rdd.py\", line 1354, in takeUpToNumLeft\n  File \"/hadoopfs11/hadoop/yarn/local/usercache/id860112/appcache/application_1580124036614_25546/container_e118_1580124036614_25546_01_000008/pyspark.zip/pyspark/serializers.py\", line 145, in load_stream\n    yield self._read_with_length(stream)\n  File \"/hadoopfs11/hadoop/yarn/local/usercache/id860112/appcache/application_1580124036614_25546/container_e118_1580124036614_25546_01_000008/pyspark.zip/pyspark/serializers.py\", line 170, in _read_with_length\n    return self.loads(obj)\n  File \"/hadoopfs11/hadoop/yarn/local/usercache/id860112/appcache/application_1580124036614_25546/container_e118_1580124036614_25546_01_000008/pyspark.zip/pyspark/serializers.py\", line 562, in loads\n    return pickle.loads(obj)\n  File \"/hadoopfs11/hadoop/yarn/local/usercache/id860112/appcache/application_1580124036614_25546/container_e118_1580124036614_25546_01_000008/pyspark.zip/pyspark/mllib/__init__.py\", line 28, in <module>\nImportError: No module named numpy\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$1.apply(PythonRDD.scala:141)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$1.apply(PythonRDD.scala:141)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:141)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor168.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/hadoopfs11/hadoop/yarn/local/usercache/id860112/appcache/application_1580124036614_25546/container_e118_1580124036614_25546_01_000008/pyspark.zip/pyspark/worker.py\", line 229, in main\n    process()\n  File \"/hadoopfs11/hadoop/yarn/local/usercache/id860112/appcache/application_1580124036614_25546/container_e118_1580124036614_25546_01_000008/pyspark.zip/pyspark/worker.py\", line 224, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/hadoopfs11/hadoop/yarn/local/usercache/id860112/appcache/application_1580124036614_25546/container_e118_1580124036614_25546_01_000008/pyspark.zip/pyspark/serializers.py\", line 372, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/opt/px/devs/miniconda/envs/spark_2.3.0/share/spark-2.3.0/python/pyspark/rdd.py\", line 1354, in takeUpToNumLeft\n  File \"/hadoopfs11/hadoop/yarn/local/usercache/id860112/appcache/application_1580124036614_25546/container_e118_1580124036614_25546_01_000008/pyspark.zip/pyspark/serializers.py\", line 145, in load_stream\n    yield self._read_with_length(stream)\n  File \"/hadoopfs11/hadoop/yarn/local/usercache/id860112/appcache/application_1580124036614_25546/container_e118_1580124036614_25546_01_000008/pyspark.zip/pyspark/serializers.py\", line 170, in _read_with_length\n    return self.loads(obj)\n  File \"/hadoopfs11/hadoop/yarn/local/usercache/id860112/appcache/application_1580124036614_25546/container_e118_1580124036614_25546_01_000008/pyspark.zip/pyspark/serializers.py\", line 562, in loads\n    return pickle.loads(obj)\n  File \"/hadoopfs11/hadoop/yarn/local/usercache/id860112/appcache/application_1580124036614_25546/container_e118_1580124036614_25546_01_000008/pyspark.zip/pyspark/mllib/__init__.py\", line 28, in <module>\nImportError: No module named numpy\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$1.apply(PythonRDD.scala:141)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$1.apply(PythonRDD.scala:141)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "model = ALS.train(ratings, rank=10, iterations=10)\n",
    "\n",
    "unrated_RDD = sc.parallelize([(1, 2), (1, 1)])\n",
    "\n",
    "predictions = model.predictAll(unrated_RDD)\n",
    "predictions.collect()\n",
    "\n",
    "preds = predictions.map(lambda x: ((x[0], x[1]), x[2]))\n",
    "preds.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T10:24:01.517173Z",
     "start_time": "2020-02-13T10:24:01.145236Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SparseVector' object has no attribute 'map'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-171-b439f6498bf4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mwords_transformed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mwords_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwords_transformed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mLabeledPoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'SparseVector' object has no attribute 'map'"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.feature import HashingTF\n",
    "sentence = \"hello hello world\"\n",
    "words = sentence.split()\n",
    "tf = HashingTF(numFeatures=100)\n",
    "words_transformed = tf.transform(words)\n",
    "\n",
    "words_samples = words_transformed.map(lambda x:LabeledPoint(1, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T10:18:29.716309Z",
     "start_time": "2020-02-13T10:18:29.697541Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LabeledPoint' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-161-662717da084f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m data = [\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mLabeledPoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mLabeledPoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m ]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LabeledPoint' is not defined"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "LabeledPoint(0.0, [0.0, 1.0]),\n",
    "LabeledPoint(1.0, [1.0, 0.0]),\n",
    "]\n",
    "RDD = sc.parallelize(data)\n",
    "\n",
    "model = LogisticRegressionWithLBFGS.train(RDD)\n",
    "\n",
    "lrm.predict([1.0, 0.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark tables & SDL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-29T14:37:36.673907Z",
     "start_time": "2019-04-29T14:37:36.413856Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                Name|Total|\n",
      "+--------------------+-----+\n",
      "|            Torterra|  525|\n",
      "|            Shieldon|  350|\n",
      "|           Bastiodon|  495|\n",
      "| WormadamSandy Cloak|  424|\n",
      "|           Vespiquen|  474|\n",
      "|            Bronzong|  500|\n",
      "|           Spiritomb|  485|\n",
      "|GarchompMega Garc...|  700|\n",
      "|           Hippowdon|  525|\n",
      "|             Drapion|  500|\n",
      "|AbomasnowMega Abo...|  594|\n",
      "|           Magnezone|  535|\n",
      "|           Rhyperior|  535|\n",
      "|           Tangrowth|  535|\n",
      "|             Leafeon|  525|\n",
      "|             Glaceon|  525|\n",
      "|             Gliscor|  510|\n",
      "|           Probopass|  525|\n",
      "|            Dusknoir|  525|\n",
      "|     RotomHeat Rotom|  520|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.registerTempTable(\"data\")\n",
    "\n",
    "# SQL statements can be run by using the sql method\n",
    "x = sqlContext.sql(\"SELECT Name, Total FROM data WHERE Generation >= 4 AND Generation <= 7 and Defense > 100\")\n",
    "x.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Close session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T10:54:40.569073Z",
     "start_time": "2020-02-13T10:54:38.195679Z"
    }
   },
   "outputs": [],
   "source": [
    "rp.sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python_2.7",
   "language": "python",
   "name": "python_2.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "341px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
