{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-06T11:56:03.641398Z",
     "start_time": "2019-11-06T11:56:01.225702Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Heart Disease UCI\n",
    "\n",
    "Source: Kaggle\n",
    "\n",
    "https://www.kaggle.com/ronitf/heart-disease-uci/downloads/heart-disease-uci.zip/1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attribute Information: \n",
    "> 1. age \n",
    "> 2. sex \n",
    "> 3. chest pain type (4 values) \n",
    "> 4. resting blood pressure \n",
    "> 5. serum cholestoral in mg/dl \n",
    "> 6. fasting blood sugar > 120 mg/dl\n",
    "> 7. resting electrocardiographic results (values 0,1,2)\n",
    "> 8. maximum heart rate achieved \n",
    "> 9. exercise induced angina \n",
    "> 10. oldpeak = ST depression induced by exercise relative to rest \n",
    "> 11. the slope of the peak exercise ST segment \n",
    "> 12. number of major vessels (0-3) colored by flourosopy \n",
    "> 13. thal: 3 = normal; 6 = fixed defect; 7 = reversable defect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T09:03:29.934180Z",
     "start_time": "2019-11-07T09:03:29.736000Z"
    }
   },
   "outputs": [],
   "source": [
    "# read the heart_cleaned data\n",
    "df = ...\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-06T11:56:03.712315Z",
     "start_time": "2019-11-06T11:56:03.700560Z"
    }
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T09:03:32.318165Z",
     "start_time": "2019-11-07T09:03:32.315206Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "factor_columns = [\"sex\", \"cp\", \"fbs\", \"restecg\", \"exang\", \"slope\", \"ca\", \"target\", \"thal\"]\n",
    "numeric_columns = [\"age\", \"trestbps\", \"chol\", \"thalach\", \"oldpeak\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we will try to predict the cholesterol of a person based on the other variables.\n",
    "\n",
    "Since it's a continuous variable, it's a perfect regression problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, let's start by preparing our target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-06T13:24:30.365975Z",
     "start_time": "2019-11-06T13:24:30.362772Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target = \"age\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T09:04:06.279580Z",
     "start_time": "2019-11-07T09:04:06.270150Z"
    }
   },
   "outputs": [],
   "source": [
    "# Isolate the target from the rest of the data in y\n",
    "y = ...\n",
    "X = ...\n",
    "print(X.head())\n",
    "\n",
    "# get a list of the columns\n",
    "feature_list = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-06T11:56:08.494793Z",
     "start_time": "2019-11-06T11:56:07.765261Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for col in numeric_columns:\n",
    "    if col != target:\n",
    "        plt.plot(X[col], y, 'o')\n",
    "        plt.title(col)\n",
    "        plt.xlabel(col)\n",
    "        plt.ylabel(target)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need to split the data into a training and testing set to be able to evaluate the quality of our model and its generalizability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hint: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-06T13:24:35.711429Z",
     "start_time": "2019-11-06T13:24:35.701033Z"
    }
   },
   "outputs": [],
   "source": [
    "# import train_test_split from sklearn\n",
    "from sklearn.... import ...\n",
    "\n",
    "# Split dataset into training and testing part\n",
    "...\n",
    "\n",
    "# Display number of rows and columns for both training and testing sets\n",
    "...\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train our model (hint: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-06T13:24:44.173044Z",
     "start_time": "2019-11-06T13:24:44.094829Z"
    }
   },
   "outputs": [],
   "source": [
    "# import Linear regression from sklearn \n",
    "from sklearn.... import ...\n",
    "\n",
    "# Instantiate model\n",
    "model = ...\n",
    "\n",
    "# Fit the model to the training set\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-06T13:24:45.593117Z",
     "start_time": "2019-11-06T13:24:45.588240Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Create a function that compute the MAE, MSE and R2 from actual and predicted values\n",
    "def evaluate_regression(actuals, preds):\n",
    "    \n",
    "    print(\"RMSE = {:.1f}\".format(...))\n",
    "    print(\"MAE = {:.1f}\".format(...))\n",
    "\n",
    "    print(\"\\nR² = {:.2f}\\n\\n\".format(...))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create predictions for training set\n",
    "y_pred_train = ...\n",
    "\n",
    "# Apply your function to see the performance\n",
    "print(\"In sample evaluation \\n---------------------\")\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-06T13:24:48.088666Z",
     "start_time": "2019-11-06T13:24:48.080667Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create predictions for test set\n",
    "y_pred = \n",
    "\n",
    "# Apply your function to see the performance\n",
    "print(\"Out sample evaluation \\n---------------------\")\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use PCA to determine the optimal number of components to get the best performance (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-06T12:52:20.681766Z",
     "start_time": "2019-11-06T12:52:20.081909Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "model = LinearRegression()\n",
    "pca = PCA()\n",
    "pipe = Pipeline(steps=[('pca', pca), ('linear', model)])\n",
    "\n",
    "# Parameters of pipelines can be set using ‘__’ separated parameter names:\n",
    "param_grid = {\n",
    "    'pca__n_components': np.arange(1,13)\n",
    "}\n",
    "search = GridSearchCV(pipe, param_grid, iid=False, cv=5,\n",
    "                      return_train_score=False)\n",
    "search.fit(X_train, y_train)\n",
    "print(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\n",
    "print(search.best_params_)\n",
    "\n",
    "# Plot the PCA spectrum\n",
    "pca.fit(X_train)\n",
    "\n",
    "fig, (ax0, ax1) = plt.subplots(nrows=2, sharex=True, figsize=(6, 6))\n",
    "ax0.plot(pca.explained_variance_ratio_, linewidth=2)\n",
    "ax0.set_ylabel('PCA explained variance')\n",
    "\n",
    "ax0.axvline(search.best_estimator_.named_steps['pca'].n_components,\n",
    "            linestyle=':', label='n_components chosen')\n",
    "ax0.legend(prop=dict(size=12))\n",
    "\n",
    "# For each number of components, find the best classifier results\n",
    "results = pd.DataFrame(search.cv_results_)\n",
    "components_col = 'param_pca__n_components'\n",
    "best_clfs = results.groupby(components_col).apply(\n",
    "    lambda g: g.nlargest(1, 'mean_test_score'))\n",
    "\n",
    "best_clfs.plot(x=components_col, y='mean_test_score', yerr='std_test_score',\n",
    "               legend=False, ax=ax1)\n",
    "ax1.set_ylabel('Classification accuracy (val)')\n",
    "ax1.set_xlabel('n_components')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hint: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-06T13:26:23.778010Z",
     "start_time": "2019-11-06T13:26:23.759174Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Instantiate the model\n",
    "model = ...\n",
    "\n",
    "# Fit the model\n",
    "...\n",
    "\n",
    "# make predictions\n",
    "preds = ...\n",
    "\n",
    "# Evaluate performance\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the feature importance\n",
    "feature_imp = ...\n",
    "print(\"Most important features : \\n\",feature_imp)\n",
    "\n",
    "# Plot the feature importance \n",
    "...\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now turn to a classification problem. We will try to predict the target value (0 or 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T09:03:35.597077Z",
     "start_time": "2019-11-07T09:03:35.593722Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target = \"target\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-06T11:57:05.411024Z",
     "start_time": "2019-11-06T11:57:04.759503Z"
    }
   },
   "outputs": [],
   "source": [
    "for y in numeric_columns:\n",
    "    sns.boxplot(x = target, y = y, data=df, orient=\"v\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-06T11:57:12.211085Z",
     "start_time": "2019-11-06T11:57:11.320402Z"
    }
   },
   "outputs": [],
   "source": [
    "for col in factor_columns:\n",
    "    if col != \"target\":\n",
    "        print(\"--- {} ---\".format(col))\n",
    "        display(pd.crosstab(df[target], df[col]).style.background_gradient())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take some time to analyze the plots above and draw some conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T09:04:19.698752Z",
     "start_time": "2019-11-07T09:04:19.683607Z"
    }
   },
   "outputs": [],
   "source": [
    "# Isolate the target from the rest of the data in y\n",
    "y = ...\n",
    "X = ...\n",
    "print(X.head())\n",
    "\n",
    "# get a list of the columns\n",
    "feature_list = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First it's important to isolate the target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In oreder to get better estimates, let's normalize the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need to split the data into a training and testing set to be able to evaluate the quality of our model and its generalizability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T09:04:22.335754Z",
     "start_time": "2019-11-07T09:04:22.329706Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split dataset into training and testing part\n",
    "...\n",
    "\n",
    "# Display number of rows and columns for both training and testing sets\n",
    "...\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T09:04:22.942497Z",
     "start_time": "2019-11-07T09:04:22.937671Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, roc_auc_score\n",
    "\n",
    "# Create a function that compute the accuracy, precision, recall, AUC and confusion matrix from actual and predicted values\n",
    "def evaluate_classification(actuals, preds):\n",
    "    \n",
    "    print(\"Accuracy = {:.1%}\".format(...))\n",
    "    print(\"Precision = {:.1%}\".format(...))\n",
    "    print(\"Recall = {:.1%}\".format(...))\n",
    "\n",
    "    print(\"AUC = {:.1%}\".format(...))\n",
    "\n",
    "    print(\"\\nConfusion matrix: \\n\",...)\n",
    "    \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hint: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T09:08:46.784205Z",
     "start_time": "2019-11-07T09:08:46.755164Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.... import ...\n",
    "\n",
    "# instantiate and fit the model to the training set\n",
    "...\n",
    "...\n",
    "\n",
    "# make predictions\n",
    "...\n",
    "\n",
    "#evaluate the model on the training set using your user defined function\n",
    "print(\"In sample evaluation \\n---------------------\")\n",
    "...\n",
    "\n",
    "# evaluate the model on the test set using your user defined function\n",
    "print(\"Out sample evaluation \\n---------------------\")\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T09:07:05.138800Z",
     "start_time": "2019-11-07T09:07:04.998108Z"
    }
   },
   "outputs": [],
   "source": [
    "# compute the score of each row and plot it along with the target prediction (hint: sort data so it's cleaner) to see the sigmoid\n",
    "proba = ...\n",
    "plt.scatter(range(len(proba)),...)\n",
    "plt.scatter(range(len(preds)), ...)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hint https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-06T11:57:34.953480Z",
     "start_time": "2019-11-06T11:57:34.726059Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.... import ...\n",
    "\n",
    "\n",
    "# instantiate and fit the model to the training set\n",
    "...\n",
    "...\n",
    "\n",
    "# make predictions\n",
    "...\n",
    "\n",
    "#evaluate the model on the training set using your user defined function\n",
    "print(\"In sample evaluation \\n---------------------\")\n",
    "...\n",
    "\n",
    "# evaluate the model on the test set using your user defined function\n",
    "print(\"Out sample evaluation \\n---------------------\")\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not too bad (in practice we would accept the model, depending on the context and risks, if recall and precision are smaller\n",
    "\n",
    "However we have here an illustration of overfitting. Indeed the performances on the training set are much better than on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now dig into the results of the model and try to understand what happened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-06T11:59:38.868178Z",
     "start_time": "2019-11-06T11:59:38.628766Z"
    }
   },
   "outputs": [],
   "source": [
    "# collect the feature importance from the model and sort by importance\n",
    "feature_imp = ...\n",
    "print(\"Most important features : \\n\",feature_imp)\n",
    "\n",
    "%matplotlib inline\n",
    "# plot a barplot of the features by decreasing importance (put a title + labels)\n",
    "...\n",
    "...\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's change some parameters to try to improve performances on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-06T12:00:25.856004Z",
     "start_time": "2019-11-06T12:00:25.839696Z"
    }
   },
   "outputs": [],
   "source": [
    "# Feel free to go in the documentation and change some of the parameters to the model to improve the performances\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could continue to try sets of parameters like this for a big amount of time but this is not very efficient, especially when you start with machine learning. \n",
    "\n",
    "Another more research-like approach is called GridSearch. This method is based on the principle that the user should create a grid of regularly spaced parameters and then test the model on each node of that grid and compare. This is a more systematic method but as you can imagine it can take a lot of time if you want to test a long list of combinations for a large datasets. That's why we also often use randomized gridsearch, where instead of running the model for all points of the grid, we only do it for some random combinations. \n",
    "\n",
    "In the next cell you will implement gridsearch yourself thanks to sklearn (https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-06T12:31:08.373971Z",
     "start_time": "2019-11-06T12:31:03.795424Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.... import ...\n",
    "\n",
    "# create a dictionnary of 2 parameters with some range. Be careful not to create too many combinations of it will take time to run it\n",
    "...\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# Instantiate the gridsearchCV using your previously defined parameters dictionary. \n",
    "...\n",
    "\n",
    "# fit the gridsearch object to the training set\n",
    "...\n",
    "\n",
    "print(\"Best parameters= {}\".format(...))\n",
    "print(\"Best score= {}\".format(...))\n",
    "\n",
    "# get the results of the gridsearch in a dataframe format and print by descending order of \"mean_test_score\"\n",
    "results = ...\n",
    "print(...)\n",
    "\n",
    "# Display the grid points and highlight the point that gave the best model performance (don't forget a title, labels and legend)\n",
    "...\n",
    "...\n",
    "\n",
    "# get the best estimator, use it to make predictions and evaluate the performances on the test set\n",
    "best_model = ...\n",
    "\n",
    "y_pred = ...\n",
    "\n",
    "print(\"Out sample evaluation \\n---------------------\")\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Optional) Feel free to experiment with other ML models defined in sklearn to get comfortable with the model creation and analysis.\n",
    "Here are some ideas:\n",
    "* KNN\n",
    "* Naive Bayes \n",
    "* SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks are very popular these days. \n",
    "\n",
    "So we will conclude this training by giving you the opportunity to create your own fully connected neural network as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T09:10:14.420341Z",
     "start_time": "2019-11-07T09:10:14.414532Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Don't modify this - just run it\n",
    "def performance_nn(model):\n",
    "    history = model.fit(X_train, y_train, \n",
    "          epochs=500, # to be modified\n",
    "          validation_split = 0.3,\n",
    "            verbose=0)\n",
    "\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['training', 'validation'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    results = model.evaluate(X_train, y_train)\n",
    "    print(\"Training Loss = {:.2f}, \\t accuracy = {:.2f} \\n\".format(results[0], results[1]))\n",
    "\n",
    "    results = model.evaluate(X_test, y_test)\n",
    "    print(\"Testing Loss = {:.2f}, accuracy  = {:.2f} \\n\".format(results[0], results[1],))\n",
    "\n",
    "    nn_preds = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hint: https://keras.io/layers/core/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T09:10:21.841825Z",
     "start_time": "2019-11-07T09:10:15.704181Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "model = Sequential()\n",
    "# add dense layers to the model (you can use any number of layers you want and as many nodes you like) \n",
    "...\n",
    "...\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "performance_nn(model);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To conclude this workshop, we will show a very basic and simple example of image recognition.\n",
    "\n",
    "In the previous section you created a first neural network with a sequential fully connected topology. But there are many more topologies which are each best suited to a particular kind of machine learning problem. For image recognition for example convolutional neural networks have been proved to work best. We won't implement such CNN here because this will be done in a more advanced training in the future but we will just explore the basics of image recognition with sklearn.\n",
    "\n",
    "To do so we will first load a different dataset containing small imaged of digits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T08:49:25.685016Z",
     "start_time": "2019-11-07T08:49:25.558977Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "df = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T08:49:26.826980Z",
     "start_time": "2019-11-07T08:49:26.282791Z"
    }
   },
   "outputs": [],
   "source": [
    "images_and_labels = list(zip(df.images, df.target))\n",
    "for index, (image, label) in enumerate(images_and_labels[:4]):\n",
    "    plt.subplot(2, 4, index + 1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.title('Training: %i' % label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T08:49:27.921770Z",
     "start_time": "2019-11-07T08:49:27.917629Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_samples = len(df.images)\n",
    "data = df.images.reshape((n_samples, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T09:01:36.561705Z",
     "start_time": "2019-11-07T09:01:34.619390Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets, neural_network, metrics\n",
    "\n",
    "# Adapt the netword topology (number of layers and number of nodes)\n",
    "classifier = neural_network.MLPClassifier(activation=\"relu\",\n",
    "                                          hidden_layer_sizes = (...),\n",
    "                                          max_iter = 3000,\n",
    "                                          random_state = 42,\n",
    "                                          verbose=False)\n",
    "\n",
    "# We learn the digits on the first half of the digits\n",
    "classifier.fit(data[:n_samples // 2], df.target[:n_samples // 2])\n",
    "\n",
    "# Now predict the value of the digit on the second half:\n",
    "expected = df.target[n_samples // 2:]\n",
    "predicted = classifier.predict(data[n_samples // 2:])\n",
    "\n",
    "print(\"Classification report for classifier %s:\\n%s\\n\"\n",
    "      % (classifier, metrics.classification_report(expected, predicted)))\n",
    "print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(expected, predicted))\n",
    "\n",
    "images_and_predictions = list(zip(df.images[n_samples // 2:], predicted))\n",
    "for index, (image, prediction) in enumerate(images_and_predictions[:8]):\n",
    "    plt.subplot(2, 4, index + 1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.title('Prediction: %i' % prediction)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.plot(np.log(classifier.loss_curve_))\n",
    "plt.ylabel(\"Log Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.title(\"Learning curve\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Going further"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This introductory workshop to machine learning showed you how - easy - creating predictive models can be.\n",
    "\n",
    "However note that we used a very simple dataset, with few cleaning steps and only numerical variables.\n",
    "\n",
    "In practice we often need to go back and forth between modeling and cleaning until we achieve desired results.\n",
    "\n",
    "Also model tuning and refinement might require some time with big datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a list of topic you could be interested in if you want to go further in unsupervised learning\n",
    "\n",
    "* up/downsampling \n",
    "* Cross valisation & randomized gridsearch\n",
    "* Advanced deep learning : RNN, CNN, GAM, LSTM, ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "341.313px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
